{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 15:19:42.514875: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 15:19:42.705640: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-17 15:19:42.929642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-17 15:19:43.179611: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-17 15:19:43.181037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-17 15:19:43.490332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 15:19:45.712353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4'\n",
    "import cv2  # OpenCV 用於處理圖片\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments.wrappers import FlattenActionWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "class MapDroneEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, map_image_path):\n",
    "        self.map_image = cv2.imread(map_image_path)  # 讀取地圖圖片\n",
    "        self.map_height, self.map_width, _ = self.map_image.shape\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')  # 動作：上、下、左、右\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(2,), dtype=np.int32, minimum=0, maximum=max(self.map_height, self.map_width), name='observation')  # 狀態：無人機座標\n",
    "\n",
    "        self._state = np.array([self.map_height // 2, self.map_width // 2], dtype=np.int32)  # 初始位置在圖片中心\n",
    "        self.goal_position = np.array([955, 317]) \n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.array([self.map_height // 2, self.map_width // 2], dtype=np.int32)\n",
    "        return ts.restart(self._state)\n",
    "\n",
    "    def _discretize_action(self, action):\n",
    "        \"\"\"將離散動作值轉換為連續動作向量\"\"\"\n",
    "        x_idx = action // 3  # 計算 x 維度的區間索引\n",
    "        y_idx = action % 3   # 計算 y 維度的區間索引\n",
    "\n",
    "        x_bins = [-1, -0.33, 0.33, 1]\n",
    "        y_bins = [-1, -0.33, 0.33, 1]\n",
    "        \n",
    "        # 限制索引範圍，以確保不會超過陣列邊界\n",
    "        x_idx = min(x_idx, len(x_bins) - 2)  \n",
    "        y_idx = min(y_idx, len(y_bins) - 2)\n",
    "        \n",
    "        x_val = (x_bins[x_idx] + x_bins[x_idx + 1]) / 2  # 取區間中點\n",
    "        y_val = (y_bins[y_idx] + y_bins[y_idx + 1]) / 2\n",
    "\n",
    "        return np.array([x_val, y_val]).round().astype(np.int32)\n",
    "    \n",
    "    def _step(self, action):\n",
    "        continuous_action = self._discretize_action(action)  # 轉換為連續動作\n",
    "\n",
    "        # 使用 continuous_action 更新位置\n",
    "        move = continuous_action.round().astype(np.int32)\n",
    "        self._state = (self._state + move).astype(np.int32)\n",
    "\n",
    "        # Clip the state to stay within the map boundaries\n",
    "        self._state = np.clip(self._state, 0, [self.map_height - 1, self.map_width - 1]).astype(np.int32)\n",
    "\n",
    "        reward = self._compute_reward()\n",
    "        done = np.array_equal(self._state, self.goal_position)\n",
    "\n",
    "        if done:\n",
    "              return ts.termination(self._state, reward)\n",
    "        else:\n",
    "              return ts.transition(self._state.astype(np.int32), reward, discount=1.0)\n",
    "    # reward function\n",
    "    def _compute_reward(self):\n",
    "        # 計算獎勵，例如根據與目標的距離給予獎勵\n",
    "        distance = np.linalg.norm(self._state - self.goal_position)\n",
    "        return -distance  # 距離越近，獎勵越高\n",
    "\n",
    "    def render(self):\n",
    "        # 可選：渲染環境，例如在圖片上標記無人機和目標位置\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_image_path = 'ccu_map.png'  # 請替換為您的地圖圖片路徑\n",
    "env = MapDroneEnv(map_image_path)\n",
    "env = tf_py_environment.TFPyEnvironment(env)  # 轉換為 TF 環境\n",
    "# env = FlattenActionWrapper(env)  # 使用包裝器\n",
    "\n",
    "# 設定目標位置（例如：中正大學圖書館）\n",
    "goal_x, goal_y = 955, 317  # 請根據地圖圖片設定目標的像素座標\n",
    "env.goal_position = np.array([goal_x, goal_y])  # 在這裡設定目標位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 DQN 代理\n",
    "fc_layer_params = (20,)\n",
    "\n",
    "# 調整 action_spec\n",
    "updated_action_spec = array_spec.BoundedArraySpec(\n",
    "      shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    updated_action_spec, # 使用更新後的 action_spec\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "class MyAdamOptimizer(tf.keras.optimizers.Adam):\n",
    "    def _zeros_slot(self, var, slot_name, op_name):\n",
    "        named_slots = self._slot_dict(slot_name)\n",
    "        if id(var.ref()) not in named_slots:  # 使用 id(var.ref()) \n",
    "            new_slot_variable = slot_creator.create_zeros_slot(var, op_name, copy_xla_sharding=True)\n",
    "            self._restore_slot_variable(slot_name=slot_name, variable=var, slot_variable=new_slot_variable)\n",
    "\n",
    "optimizer = MyAdamOptimizer(learning_rate=1e-3)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立觀察者 (Observer)\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立收集數據的策略\n",
    "initial_collect_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(), env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       "  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-923.06555], dtype=float32)>,\n",
       "  'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       "  'observation': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[210, 862]], dtype=int32)>}),\n",
       " ())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 收集初始數據\n",
    "init_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer_observer],\n",
    "    num_steps=200)  # 或者其他您認為合適的初始步數\n",
    "init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立訓練的策略\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer],\n",
    "    num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/koshino17/anaconda3/envs/GPy/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# 訓練代理\n",
    "num_iterations = 10  # 迭代次數\n",
    "batch_size = 32\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/koshino17/anaconda3/envs/GPy/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 1: loss = 706221.375\n",
      "step = 2: loss = 717196.375\n",
      "step = 3: loss = 724086.8125\n",
      "step = 4: loss = 717335.6875\n",
      "step = 5: loss = 714578.75\n",
      "step = 6: loss = 733973.5\n",
      "step = 7: loss = 737691.5\n",
      "step = 8: loss = 732823.0625\n",
      "step = 9: loss = 718734.75\n",
      "step = 10: loss = 730540.25\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_driver.run()\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存訓練損失以便視覺化\n",
    "train_losses = []\n",
    "episode_rewards = []  # 儲存每個 episode 的總獎勵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_driver.run()\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    time_step = env.current_time_step()\n",
    "    episode_reward = 0\n",
    "    while not time_step.is_last():\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = env.step(action_step.action)\n",
    "        episode_reward += time_step.reward\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    train_losses.append(train_loss)  # 將損失添加到列表中\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "    # 平滑獎勵\n",
    "    df_rewards = pd.DataFrame({'episode': np.arange(len(episode_rewards)), 'reward': episode_rewards})\n",
    "    window_size = 10  # 移動平均的窗口大小\n",
    "    df_rewards['smoothed_reward'] = df_rewards['reward'].rolling(window=window_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製損失曲線\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "\n",
    "# 繪製獎勵曲線\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df_rewards['episode'], df_rewards['reward'], label='Reward')\n",
    "plt.plot(df_rewards['episode'], df_rewards['smoothed_reward'], label='Smoothed Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製策略熱力圖 v1\n",
    "inner_env = env.pyenv.envs[0]\n",
    "x_coords = np.arange(inner_env.map_width)\n",
    "y_coords = np.arange(inner_env.map_height)\n",
    "X, Y = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "actions = np.zeros_like(X)\n",
    "for i in range(inner_env.map_height):\n",
    "    for j in range(inner_env.map_width):\n",
    "        state = np.array([i, j], dtype=np.int32)\n",
    "        time_step = ts.restart(tf.expand_dims(state, axis=0))  # 在這裡添加 expand_dims\n",
    "        policy_step = agent.policy.action(time_step)\n",
    "        actions[i, j] = policy_step.action.numpy()\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis', 9)\n",
    "colors = cmap(actions)\n",
    "\n",
    "plt.imshow(colors)\n",
    "plt.title('Policy Heatmap')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.colorbar(label='Action')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import get_cmap\n",
    "import threading\n",
    "\n",
    "def calculate_action(state, actions, row, col, agent):\n",
    "    time_step = ts.restart(tf.expand_dims(state, axis=0))\n",
    "    policy_step = agent.policy.action(time_step)\n",
    "    actions[row, col] = policy_step.action.numpy()[0]\n",
    "\n",
    "# 繪製策略熱力圖\n",
    "inner_env = env.pyenv.envs[0]\n",
    "x_coords = np.arange(inner_env.map_width)\n",
    "y_coords = np.arange(inner_env.map_height)\n",
    "X, Y = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "actions = np.zeros_like(X)\n",
    "\n",
    "threads = []\n",
    "for i in range(inner_env.map_height):\n",
    "    for j in range(inner_env.map_width):\n",
    "        state = np.array([i, j], dtype=np.int32)\n",
    "        thread = threading.Thread(target=calculate_action, args=(state, actions, i, j, agent))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "# 等待所有線程完成\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "cmap = get_cmap('viridis', 9)\n",
    "colors = cmap(actions)\n",
    "\n",
    "plt.imshow(colors)\n",
    "plt.title('Policy Heatmap')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.colorbar(label='Action')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用來找座標點\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_click_coordinates(event):\n",
    "    x, y = event.xdata, event.ydata\n",
    "    if event.button == 1:  # 1 代表滑鼠左鍵\n",
    "        print(f\"Clicked coordinates: ({int(x)}, {int(y)})\")\n",
    "\n",
    "# 讀取地圖圖片\n",
    "img = plt.imread('ccu_map.png')\n",
    "\n",
    "# 顯示圖片\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "# 連接滑鼠點擊事件\n",
    "fig.canvas.mpl_connect('button_press_event', get_click_coordinates)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPy",
   "language": "python",
   "name": "gpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
